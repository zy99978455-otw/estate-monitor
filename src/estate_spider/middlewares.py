# src/estate_spider/middlewares.py

import requests
import logging

class ProxyMiddleware:
    def __init__(self, proxy_url):
        self.proxy_url = proxy_url
        self.logger = logging.getLogger(__name__)

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            proxy_url=crawler.settings.get('PROXY_POOL_URL')
        )

    def process_request(self, request, spider):
        # å¦‚æœè¯·æ±‚å·²ç»æœ‰äº†ä»£ç†ï¼ˆæ¯”å¦‚ retry é‡è¯•æ—¶å¯èƒ½å·²ç»æœ‰äº†ï¼‰ï¼Œè·³è¿‡
        if request.meta.get('proxy'):
            return

        try:
            # 1. è¯·æ±‚ Docker å®¹å™¨é‡Œçš„æ¥å£
            response = requests.get(f"{self.proxy_url}/get/", timeout=5)
            
            if response.status_code == 200:
                res_json = response.json()
                # jhao104 çš„è¿”å›æ ¼å¼ä¸­ï¼Œip åœ¨ "proxy" å­—æ®µé‡Œ
                proxy = res_json.get("proxy")
                
                if proxy:
                    # 2. æ‹¼æ¥åè®®å¤´ (è´å£³æ˜¯ HTTPSï¼Œè¿™é‡Œå¿…é¡»æ³¨æ„)
                    # å…è´¹ä»£ç†å¤§å¤šæ•°åªæ”¯æŒ HTTPï¼Œä½†éƒ¨åˆ†èƒ½éš§é“è½¬å‘ HTTPS
                    # æˆ‘ä»¬å…ˆè®¾ä¸º http://ï¼Œè®© Scrapy è‡ªå·±å» connect
                    request.meta['proxy'] = f"http://{proxy}"
                    self.logger.debug(f"ğŸ›¡ï¸ [ProxyMiddleware] è£…å¤‡ä»£ç†: {proxy}")
                else:
                    self.logger.warning("âš ï¸ ä»£ç†æ± æ¥å£è¿”å›ç©ºï¼Œæ­£åœ¨è£¸å¥”ï¼")
        except Exception as e:
            self.logger.error(f"âŒ è¿æ¥ä»£ç†æ± æœåŠ¡å¤±è´¥: {e}")
            # å¤±è´¥äº†ä¸è¦æŠ›å¼‚å¸¸ï¼Œè®©å®ƒç»§ç»­ç”¨æœ¬æœº IP è·‘ï¼Œä¿è¯ç¨³å®šæ€§