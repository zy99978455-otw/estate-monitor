import scrapy
import json
from scrapy.http import HtmlResponse
from ..items import EstateItem

class BeikeSpider(scrapy.Spider):
    name = "beike"
    allowed_domains = ["cd.ke.com"]
    
    # ============================================================
    # ğŸ”§ æœ€ç»ˆç¡®è®¤é…ç½®
    # ============================================================
    
    # 1. åŒºåŸŸåŸºå‡†åœ°å€ (æ£•åŒ—)
    BASE_URL = "https://cd.ke.com/ershoufang/zongbei/"
    
    # 2. ç­›é€‰å‚æ•° (å®Œå…¨æŒ‰ç…§ä½ çš„ URL)
    # co32: æœ€æ–°å‘å¸ƒ
    # l2l3: äºŒå®¤ã€ä¸‰å®¤
    # p5:   ä»·æ ¼åŒºé—´ (100-150ä¸‡)
    FILTER_CODE = "co32l2l3p5"

    def start_requests(self):
            # ç¬¬ 1 é¡µ URL: https://cd.ke.com/ershoufang/zongbei/co32l2l3p5/
            first_page_url = f"{self.BASE_URL}{self.FILTER_CODE}/"
            
            yield scrapy.Request(
                first_page_url,
                meta={
                    "playwright": True,
                    "playwright_include_page": True,
                    "is_first_page": True, # å¼€å¯ç¬¬ä¸€é¡µäººå·¥éªŒè¯
                },
                callback=self.parse
            )

    async def parse(self, response):
        page = response.meta["playwright_page"]
        is_first_page = response.meta.get("is_first_page", False)

        # ============================================================
        # ğŸ›‘ ç¬¬ä¸€é¡µä¸“å±é€»è¾‘ï¼šäººå·¥å¹²é¢„ (åªåœ¨ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶è§¦å‘)
        # ============================================================
        if is_first_page:
            print("="*60)
            print(f"ğŸ•µï¸â€â™‚ï¸ [Playwright] æµè§ˆå™¨å·²å¯åŠ¨ï¼æ­£åœ¨æ¸²æŸ“ç¬¬ä¸€é¡µ...")
            print("ğŸš¨ğŸš¨ğŸš¨ ã€äººå·¥å¹²é¢„æ—¶åˆ»ã€‘ ğŸš¨ğŸš¨ğŸš¨")
            print("1. è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ã€‚")
            print("2. ç¡®è®¤æµè§ˆå™¨é‡Œå·²ç»å‡ºç°äº†ã€æˆ¿æºåˆ—è¡¨ã€‘ã€‚")
            print("3. ç¡®è®¤æ— è¯¯åï¼Œä»£ç å°†è‡ªåŠ¨æ£€æµ‹å¹¶å¼€å§‹è·‘å…¨é‡æ•°æ®...")
            print("="*60)

            # æ™ºèƒ½è½®è¯¢ç­‰å¾… (æœ€å¤šç­‰ 120ç§’)
            for i in range(20):
                try:
                    # å°è¯•å¯»æ‰¾åˆ†é¡µæ•°æ® div (è¿™æ ‡å¿—ç€é¡µé¢åŠ è½½å®Œå…¨æˆåŠŸ)
                    await page.wait_for_selector('div.house-lst-page-box', timeout=6000)
                    print("ğŸ‰ æ£€æµ‹åˆ°é¡µé¢åŠ è½½æˆåŠŸï¼å¼€å§‹è‡ªåŠ¨åŒ–é‡‡é›†...")
                    break
                except:
                    print(f"â³ ({i+1}/20) ç­‰å¾…äººå·¥è¿‡éªŒè¯ç ...")
                    # æ¨¡æ‹Ÿé¼ æ ‡è½»å¾®æ™ƒåŠ¨ï¼Œé˜²æ­¢è¢«åˆ¤å®šä¸ºæ­»é“¾æ¥
                    await page.mouse.move(100, 100)
        else:
            # åç»­é¡µé¢ï¼šåªéœ€è¦ç®€å•ç­‰å¾…åˆ—è¡¨å‡ºç°å³å¯
            print(f"ğŸ”„ [è‡ªåŠ¨ç¿»é¡µ] æ­£åœ¨æŠ“å–: {response.url}")
            try:
                await page.wait_for_selector('ul.sellListContent', timeout=10000)
            except:
                print(f"âš ï¸ è­¦å‘Š: é¡µé¢åŠ è½½è¶…æ—¶ {response.url}")

        # ============================================================
        # ğŸ“¥ æ•°æ®æå–é€»è¾‘
        # ============================================================
        content = await page.content()
        await page.close() # æŠ“å®Œå½“å‰é¡µå°±å…³é—­è¿™ä¸ª Page tab
        
        response = HtmlResponse(url=response.url, body=content, encoding='utf-8')
        house_list = response.css('ul.sellListContent li.clear')

        print(f"âœ… [æœ¬é¡µæ•°æ®] æå–åˆ° {len(house_list)} æ¡æˆ¿æº")

        for house in house_list:
            item = EstateItem()
            item['title'] = house.css('.title a::text').get()
            item['detail_url'] = house.css('.title a::attr(href)').get()
            item['community'] = house.css('.positionInfo a::text').re_first(r'(.+)')
            
            position_info = house.css('.positionInfo a::text').getall()
            item['region'] = "-".join(position_info[1:]) if len(position_info) > 1 else ""
            
            # âœ¨ ä¿®å¤ï¼šåˆå¹¶ houseInfo ä¸‹çš„æ‰€æœ‰æ–‡æœ¬ï¼Œé˜²æ­¢æŠ“åˆ°ç©ºå€¼
            info_texts = house.css('.houseInfo *::text').getall()
            item['house_info'] = "".join(info_texts).strip()
            
            item['total_price'] = house.css('.totalPrice span::text').get()
            item['unit_price'] = house.css('.unitPrice span::text').get()

            yield item

        # ============================================================
        # ğŸ“‘ è‡ªåŠ¨ç¿»é¡µé€»è¾‘ (åªåœ¨ç¬¬ä¸€é¡µè§¦å‘)
        # ============================================================
        if is_first_page:
            # 1. ä» HTML ä¸­æå–åˆ†é¡µæ•°æ®
            # è´å£³çš„åˆ†é¡µä¿¡æ¯åœ¨ <div class="page-box ..." page-data='{"totalPage":5,"curPage":1}'>
            page_data_str = response.css('div.house-lst-page-box::attr(page-data)').get()
            
            if page_data_str:
                try:
                    page_data = json.loads(page_data_str)
                    total_page = page_data.get("totalPage", 0)
                    print(f"ğŸ“š [åˆ†é¡µåˆ†æ] å…±æ£€æµ‹åˆ° {total_page} é¡µæ•°æ®ï¼Œå¼€å§‹ç”Ÿæˆåç»­ä»»åŠ¡...")

                    # 2. å¾ªç¯ç”Ÿæˆåç»­é¡µé¢çš„ Request
                    # ä»ç¬¬ 2 é¡µ åˆ° ç¬¬ total_page é¡µ
                    for page_num in range(2, total_page + 1):
                        # æ„é€  URL: .../zongbei/pg2co32l2l3p5/
                        next_url = f"{self.BASE_URL}pg{page_num}{self.FILTER_CODE}/"
                        
                        yield scrapy.Request(
                            next_url,
                            meta={
                                "playwright": True,
                                "playwright_include_page": True,
                                "is_first_page": False, # ğŸš© æ ‡è®°ï¼šåç»­é¡µé¢ä¸éœ€è¦äººå·¥å¹²é¢„
                            },
                            callback=self.parse
                        )
                except Exception as e:
                    print(f"âŒ è§£æåˆ†é¡µæ•°æ®å¤±è´¥: {e}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°åˆ†é¡µæ•°æ®ï¼Œå¯èƒ½åªæœ‰ 1 é¡µï¼Œæˆ–è¢«åçˆ¬æ‹¦æˆªã€‚")