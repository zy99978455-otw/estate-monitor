import scrapy
import json
from scrapy.http import HtmlResponse
# å‡è®¾ä½ çš„ items.py åœ¨ä¸Šä¸€çº§ç›®å½•
from ..items import EstateItem 

class BeikeSpider(scrapy.Spider):
    name = "beike"
    allowed_domains = ["cd.ke.com"]
    
    BASE_URL = "https://cd.ke.com/ershoufang/"
    REGIONS = ["zongbei", "yulin"]
    FILTER_CODE = "co32l2l3p5"

    def start_requests(self):
        # 1. éå†åŒºåŸŸåˆ—è¡¨ (ä¿®å¤ URL æ‹¼æ¥é”™è¯¯)
        for region in self.REGIONS:
            # æ­£ç¡®æ„é€ : .../ershoufang/zongbei/co32.../
            first_page_url = f"{self.BASE_URL}{region}/{self.FILTER_CODE}/"
            
            yield scrapy.Request(
                first_page_url,
                meta={
                    "playwright": True,
                    "playwright_include_page": True,
                    "is_first_page": True,
                    # å…³é”®ä¿®å¤ï¼šä¼ é€’å½“å‰åŒºåŸŸï¼Œç”¨äºåç»­ç¿»é¡µæ‹¼æ¥
                    "current_region": region, 
                    # å…³é”®ä¿®å¤ï¼šæŒ‡å®š context_nameï¼Œè®©æ‰€æœ‰é¡µé¢å…±äº«åŒä¸€ä¸ªæµè§ˆå™¨Session (ä¿ç•™éªŒè¯ç Cookie)
                    "playwright_context": "persistent_context", 
                },
                callback=self.parse
            )

    async def parse(self, response):
        page = response.meta["playwright_page"]
        is_first_page = response.meta.get("is_first_page", False)
        # å–å‡ºå½“å‰åŒºåŸŸ
        current_region = response.meta.get("current_region")

        # --- ç¬¬ä¸€é¡µäººå·¥å¹²é¢„é€»è¾‘ ---
        if is_first_page:
            print("="*60)
            print(f"ğŸ•µï¸â€â™‚ï¸ [æ­£åœ¨åˆå§‹åŒ–åŒºåŸŸ]: {current_region}")
            print("ğŸš¨ å¦‚é‡éªŒè¯ç ï¼Œè¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆï¼")
            print("="*60)

            try:
                # ç­‰å¾…åˆ—è¡¨å®¹å™¨å‡ºç°
                await page.wait_for_selector('div.house-lst-page-box', timeout=60000) # å»¶é•¿åˆ°60ç§’ç»™äººå·¥ç•™æ—¶é—´
                print("ğŸ‰ é¡µé¢åŠ è½½æˆåŠŸï¼")
            except:
                print("âš ï¸ ç­‰å¾…è¶…æ—¶ï¼Œå¯èƒ½è¢«åçˆ¬æˆ–åŠ è½½å¤±è´¥")

        else:
            print(f"ğŸ”„ [ç¿»é¡µä¸­] {current_region} - {response.url}")
            try:
                await page.wait_for_selector('ul.sellListContent', timeout=10000)
            except:
                pass

        # --- æå–æ•°æ® ---
        content = await page.content()
        await page.close() # å…³é—­å½“å‰é¡µç­¾
        
        # é‡æ–°å°è£… Response
        response = HtmlResponse(url=response.url, body=content, encoding='utf-8')
        house_list = response.css('ul.sellListContent li.clear')
        
        print(f"âœ… [{current_region}] æå–åˆ° {len(house_list)} æ¡æˆ¿æº")

        for house in house_list:
            item = EstateItem()
            item['title'] = house.css('.title a::text').get()
            item['detail_url'] = house.css('.title a::attr(href)').get()
            item['community'] = house.css('.positionInfo a::text').re_first(r'(.+)')
            
            position_info = house.css('.positionInfo a::text').getall()
            item['region'] = "-".join(position_info[1:]) if len(position_info) > 1 else ""
            
            # æ¸…ç†æ¢è¡Œç¬¦
            raw_info = "".join(house.css('.houseInfo *::text').getall())
            item['house_info'] = raw_info.replace("\n", "").strip()
            
            item['total_price'] = house.css('.totalPrice span::text').get()
            item['unit_price'] = house.css('.unitPrice span::text').get()
            
            yield item

        # --- è‡ªåŠ¨ç¿»é¡µé€»è¾‘ ---
        if is_first_page:
            page_data_str = response.css('div.house-lst-page-box::attr(page-data)').get()
            
            if page_data_str:
                try:
                    page_data = json.loads(page_data_str)
                    total_page = page_data.get("totalPage", 0)
                    print(f"ğŸ“š [{current_region}] å…± {total_page} é¡µï¼Œå¼€å§‹ç”Ÿæˆä»»åŠ¡...")

                    for page_num in range(2, total_page + 1):
                        # ä¿®å¤ URLï¼šå¸¦ä¸Š current_region
                        # æ ¼å¼: .../ershoufang/zongbei/pg2co32.../
                        next_url = f"{self.BASE_URL}{current_region}/pg{page_num}{self.FILTER_CODE}/"
                        
                        yield scrapy.Request(
                            next_url,
                            meta={
                                "playwright": True,
                                "playwright_include_page": True,
                                "is_first_page": False,
                                "current_region": current_region, # ä¼ é€’åŒºåŸŸä¿¡æ¯
                                "playwright_context": "persistent_context", # ä¿æŒ Session
                            },
                            callback=self.parse
                        )
                except Exception as e:
                    print(f"âŒ åˆ†é¡µè§£æé”™è¯¯: {e}")
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°åˆ†é¡µä¿¡æ¯")